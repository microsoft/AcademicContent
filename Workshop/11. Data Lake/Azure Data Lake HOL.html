<!DOCTYPE html>
<html>
<head>
<title>Azure Data Lake HOL</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">
/* GitHub stylesheet for MarkdownPad (http://markdownpad.com) */
/* Author: Nicolas Hery - http://nicolashery.com */
/* Version: b13fe65ca28d2e568c6ed5d7f06581183df8f2ff */
/* Source: https://github.com/nicolahery/markdownpad-github */

/* RESET
=============================================================================*/

html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video {
  margin: 0;
  padding: 0;
  border: 0;
}

/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

body>*:first-child {
  margin-top: 0 !important;
}

body>*:last-child {
  margin-bottom: 0 !important;
}

/* BLOCKS
=============================================================================*/

p, blockquote, ul, ol, dl, table, pre {
  margin: 15px 0;
}

/* HEADERS
=============================================================================*/

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
}

h1 tt, h1 code, h2 tt, h2 code, h3 tt, h3 code, h4 tt, h4 code, h5 tt, h5 code, h6 tt, h6 code {
  font-size: inherit;
}

h1 {
  font-size: 28px;
  color: #000;
}

h2 {
  font-size: 24px;
  border-bottom: 1px solid #ccc;
  color: #000;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777;
  font-size: 14px;
}

body>h2:first-child, body>h1:first-child, body>h1:first-child+h2, body>h3:first-child, body>h4:first-child, body>h5:first-child, body>h6:first-child {
  margin-top: 0;
  padding-top: 0;
}

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0;
}

h1+p, h2+p, h3+p, h4+p, h5+p, h6+p {
  margin-top: 10px;
}

/* LINKS
=============================================================================*/

a {
  color: #4183C4;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

/* LISTS
=============================================================================*/

ul, ol {
  padding-left: 30px;
}

ul li > :first-child, 
ol li > :first-child, 
ul li ul:first-of-type, 
ol li ol:first-of-type, 
ul li ol:first-of-type, 
ol li ul:first-of-type {
  margin-top: 0px;
}

ul ul, ul ol, ol ol, ol ul {
  margin-bottom: 0;
}

dl {
  padding: 0;
}

dl dt {
  font-size: 14px;
  font-weight: bold;
  font-style: italic;
  padding: 0;
  margin: 15px 0 5px;
}

dl dt:first-child {
  padding: 0;
}

dl dt>:first-child {
  margin-top: 0px;
}

dl dt>:last-child {
  margin-bottom: 0px;
}

dl dd {
  margin: 0 0 15px;
  padding: 0 15px;
}

dl dd>:first-child {
  margin-top: 0px;
}

dl dd>:last-child {
  margin-bottom: 0px;
}

/* CODE
=============================================================================*/

pre, code, tt {
  font-size: 12px;
  font-family: Consolas, "Liberation Mono", Courier, monospace;
}

code, tt {
  margin: 0 0px;
  padding: 0px 0px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px;
}

pre>code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent;
}

pre {
  background-color: #f8f8f8;
  border: 1px solid #ccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px;
}

pre code, pre tt {
  background-color: transparent;
  border: none;
}

kbd {
    -moz-border-bottom-colors: none;
    -moz-border-left-colors: none;
    -moz-border-right-colors: none;
    -moz-border-top-colors: none;
    background-color: #DDDDDD;
    background-image: linear-gradient(#F1F1F1, #DDDDDD);
    background-repeat: repeat-x;
    border-color: #DDDDDD #CCCCCC #CCCCCC #DDDDDD;
    border-image: none;
    border-radius: 2px 2px 2px 2px;
    border-style: solid;
    border-width: 1px;
    font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
    line-height: 10px;
    padding: 1px 4px;
}

/* QUOTES
=============================================================================*/

blockquote {
  border-left: 4px solid #DDD;
  padding: 0 15px;
  color: #777;
}

blockquote>:first-child {
  margin-top: 0px;
}

blockquote>:last-child {
  margin-bottom: 0px;
}

/* HORIZONTAL RULES
=============================================================================*/

hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  border-bottom: 4px solid #ddd;
  padding: 0;
}

/* TABLES
=============================================================================*/

table th {
  font-weight: bold;
}

table th, table td {
  border: 1px solid #ccc;
  padding: 6px 13px;
}

table tr {
  border-top: 1px solid #ccc;
  background-color: #fff;
}

table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

/* IMAGES
=============================================================================*/

img {
  max-width: 100%
}
</style>
<style type="text/css">
.highlight  { background: #ffffff; }
.highlight .c { color: #999988; font-style: italic } /* Comment */
.highlight .err { color: #a61717; background-color: #e3d2d2 } /* Error */
.highlight .k { font-weight: bold } /* Keyword */
.highlight .o { font-weight: bold } /* Operator */
.highlight .cm { color: #999988; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #999999; font-weight: bold } /* Comment.Preproc */
.highlight .c1 { color: #999988; font-style: italic } /* Comment.Single */
.highlight .cs { color: #999999; font-weight: bold; font-style: italic } /* Comment.Special */
.highlight .gd { color: #000000; background-color: #ffdddd } /* Generic.Deleted */
.highlight .gd .x { color: #000000; background-color: #ffaaaa } /* Generic.Deleted.Specific */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #aa0000 } /* Generic.Error */
.highlight .gh { color: #999999 } /* Generic.Heading */
.highlight .gi { color: #000000; background-color: #ddffdd } /* Generic.Inserted */
.highlight .gi .x { color: #000000; background-color: #aaffaa } /* Generic.Inserted.Specific */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #555555 } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #aaaaaa } /* Generic.Subheading */
.highlight .gt { color: #aa0000 } /* Generic.Traceback */
.highlight .kc { font-weight: bold } /* Keyword.Constant */
.highlight .kd { font-weight: bold } /* Keyword.Declaration */
.highlight .kp { font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #445588; font-weight: bold } /* Keyword.Type */
.highlight .m { color: #009999 } /* Literal.Number */
.highlight .s { color: #d14 } /* Literal.String */
.highlight .na { color: #008080 } /* Name.Attribute */
.highlight .nb { color: #0086B3 } /* Name.Builtin */
.highlight .nc { color: #445588; font-weight: bold } /* Name.Class */
.highlight .no { color: #008080 } /* Name.Constant */
.highlight .ni { color: #800080 } /* Name.Entity */
.highlight .ne { color: #990000; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #990000; font-weight: bold } /* Name.Function */
.highlight .nn { color: #555555 } /* Name.Namespace */
.highlight .nt { color: #000080 } /* Name.Tag */
.highlight .nv { color: #008080 } /* Name.Variable */
.highlight .ow { font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mf { color: #009999 } /* Literal.Number.Float */
.highlight .mh { color: #009999 } /* Literal.Number.Hex */
.highlight .mi { color: #009999 } /* Literal.Number.Integer */
.highlight .mo { color: #009999 } /* Literal.Number.Oct */
.highlight .sb { color: #d14 } /* Literal.String.Backtick */
.highlight .sc { color: #d14 } /* Literal.String.Char */
.highlight .sd { color: #d14 } /* Literal.String.Doc */
.highlight .s2 { color: #d14 } /* Literal.String.Double */
.highlight .se { color: #d14 } /* Literal.String.Escape */
.highlight .sh { color: #d14 } /* Literal.String.Heredoc */
.highlight .si { color: #d14 } /* Literal.String.Interpol */
.highlight .sx { color: #d14 } /* Literal.String.Other */
.highlight .sr { color: #009926 } /* Literal.String.Regex */
.highlight .s1 { color: #d14 } /* Literal.String.Single */
.highlight .ss { color: #990073 } /* Literal.String.Symbol */
.highlight .bp { color: #999999 } /* Name.Builtin.Pseudo */
.highlight .vc { color: #008080 } /* Name.Variable.Class */
.highlight .vg { color: #008080 } /* Name.Variable.Global */
.highlight .vi { color: #008080 } /* Name.Variable.Instance */
.highlight .il { color: #009999 } /* Literal.Number.Integer.Long */
.pl-c {
    color: #969896;
}

.pl-c1,.pl-mdh,.pl-mm,.pl-mp,.pl-mr,.pl-s1 .pl-v,.pl-s3,.pl-sc,.pl-sv {
    color: #0086b3;
}

.pl-e,.pl-en {
    color: #795da3;
}

.pl-s1 .pl-s2,.pl-smi,.pl-smp,.pl-stj,.pl-vo,.pl-vpf {
    color: #333;
}

.pl-ent {
    color: #63a35c;
}

.pl-k,.pl-s,.pl-st {
    color: #a71d5d;
}

.pl-pds,.pl-s1,.pl-s1 .pl-pse .pl-s2,.pl-sr,.pl-sr .pl-cce,.pl-sr .pl-sra,.pl-sr .pl-sre,.pl-src,.pl-v {
    color: #df5000;
}

.pl-id {
    color: #b52a1d;
}

.pl-ii {
    background-color: #b52a1d;
    color: #f8f8f8;
}

.pl-sr .pl-cce {
    color: #63a35c;
    font-weight: bold;
}

.pl-ml {
    color: #693a17;
}

.pl-mh,.pl-mh .pl-en,.pl-ms {
    color: #1d3e81;
    font-weight: bold;
}

.pl-mq {
    color: #008080;
}

.pl-mi {
    color: #333;
    font-style: italic;
}

.pl-mb {
    color: #333;
    font-weight: bold;
}

.pl-md,.pl-mdhf {
    background-color: #ffecec;
    color: #bd2c00;
}

.pl-mdht,.pl-mi1 {
    background-color: #eaffea;
    color: #55a532;
}

.pl-mdr {
    color: #795da3;
    font-weight: bold;
}

.pl-mo {
    color: #1d3e81;
}
.task-list {
padding-left:10px;
margin-bottom:0;
}

.task-list li {
    margin-left: 20px;
}

.task-list-item {
list-style-type:none;
padding-left:10px;
}

.task-list-item label {
font-weight:400;
}

.task-list-item.enabled label {
cursor:pointer;
}

.task-list-item+.task-list-item {
margin-top:3px;
}

.task-list-item-checkbox {
display:inline-block;
margin-left:-20px;
margin-right:3px;
vertical-align:1px;
}
</style>
</head>
<body>
<p><a name="HOLTitle"></a></p>
<h1>Handling Big-Data Workloads with Azure Data Lake</h1>
<hr>
<p><a name="Overview"></a></p>
<h2>Overview</h2>
<p><a href="https://azure.microsoft.com/en-us/solutions/data-lake/">Azure Data Lake</a> enables you to collect data of any size, type, and velocity in one place in order to explore, analyze, and process the data using tools and languages such as U-SQL, Apache Spark, Hive, HBase, and Storm. It works with existing IT investments for identity, management, and security for simplified handling and governance. It also integrates easily with operational stores and data warehouses.</p>
<p>Data Lake consists of two primary elements: <a href="https://azure.microsoft.com/en-us/services/data-lake-store/">Azure Data Lake Store</a> and <a href="https://azure.microsoft.com/en-us/services/data-lake-analytics/">Azure Data Lake Analytics</a>. Data Lake Store is an enterprise-wide hyper-scale repository for big-data analytical workloads. It was built from the ground up to support massive throughput and integrates with Apache Hadoop by acting as an HDFS distributed file system. It also supports <a href="https://www.microsoft.com/en-us/cloud-platform/azure-active-directory">Azure Active Directory</a> for access control independent of Hadoop. Data Lake Analytics is an easy-to-learn query and analytics engine that features a new query language called U-SQL, which combines elements of traditional SQL syntax with powerful expression support and extensibility. It integrates seamlessly with Data Lake Store so you can execute queries against multiple disparate data sources as if they were one. This lab will introduce Data Lake Store and Data Lake Analytics and walk you through typical usage scenarios for each.</p>
<p><a name="Objectives"></a></p>
<h3>Objectives</h3>
<p>In this hands-on lab, you will learn how to:</p>
<ul>
<li>Create Data Lake Stores</li>
<li>Create Data Lake Analytics accounts and connect them to Data Lake Stores</li>
<li>Import data into Azure Data Lake Stores</li>
<li>Run U-SQL jobs in Azure Data Lake Analytics</li>
<li>Federate Azure SQL Databases and query them with U-SQL</li>
</ul>
<p><a name="Prerequisites"></a></p>
<h3>Prerequisites</h3>
<p>The following are required to complete this hands-on lab:</p>
<ul>
<li>An active Microsoft Azure subscription. If you don't have one, <a href="http://aka.ms/WATK-FreeTrial">sign up for a free trial</a>.</li>
<li><a href="https://azure.microsoft.com/en-us/documentation/articles/xplat-cli-install/">Azure Command-Line Interface (CLI)</a></li>
</ul>
<hr>
<p><a name="Exercises"></a></p>
<h2>Exercises</h2>
<p>This hands-on lab includes the following exercises:</p>
<ul>
<li><a href="#Exercise1">Exercise 1: Create an Azure Data Lake Store</a></li>
<li><a href="#Exercise2">Exercise 2: Create an Azure Data Lake Analytics account</a></li>
<li><a href="#Exercise3">Exercise 3: Import data into Azure Data Lake Store</a></li>
<li><a href="#Exercise4">Exercise 4: Query a TSV file with U-SQL</a></li>
<li><a href="#Exercise5">Exercise 5: Create an Azure SQL Database as a federated data source</a></li>
<li><a href="#Exercise6">Exercise 6: Perform a federated query with U-SQL</a></li>
</ul>
<p>Estimated time to complete this lab: <strong>60</strong> minutes.</p>
<p><a name="Exercise1"></a></p>
<h2>Exercise 1: Create an Azure Data Lake Store</h2>
<p>The starting point for using Azure Data Lake is setting up an Azure Data Lake Store to serve as a repository for various data sources. In this exercise, you will create a new Azure Data Lake Store in your Azure subscription. Later, you will import data into the Data Lake Store and query it with U-SQL.</p>
<ol>
<li>
<p>In your browser, navigate to the <a href="https://portal.azure.com">Azure Portal</a>. If you are asked to sign in, do so using your Microsoft account.</p>
</li>
<li>
<p>In the portal, click <strong>+ New</strong>, followed by <strong>Intelligence + analytics</strong> and <strong>Data Lake Store</strong>.</p>
<p><a href="Images/new-data-lake-store.png" target="_blank"><img src="Images/new-data-lake-store.png" alt="Adding a new Data Lake Store" style="max-width:100%;"></a></p>
<p><em>Adding a new Data Lake Store</em></p>
</li>
<li>
<p>In the "New Data Lake Store" blade, enter a unique name for your Data Lake Store in all lowercase. The name must be unique within Azure since it becomes part of the store's DNS name. Make sure <strong>Create new</strong> is selected under <strong>Resource Group</strong>, and then enter a resource-group name such as "DataLakeResourceGroup" (without quotation marks). Choose the location nearest you, and then click <strong>Create</strong>.</p>
<blockquote>
<p>If there are any input errors, such as spaces in the resource-group name, the offending fields will be flagged with red exclamation points rather than green check marks. Hover the mouse cursor over an exclamation point for help resolving the error.</p>
</blockquote>
<p><a href="Images/create-data-lake-store.png" target="_blank"><img src="Images/create-data-lake-store.png" alt="Creating a Data Lake Store" style="max-width:100%;"></a></p>
<p><em>Creating a Data Lake Store</em></p>
</li>
<li>
<p>Click <strong>Resource groups</strong> in the ribbon on the left, and then click the resource group whose name you specified in the previous step.</p>
<p><a href="Images/open-resource-group.png" target="_blank"><img src="Images/open-resource-group.png" alt="Opening the resource group" style="max-width:100%;"></a></p>
<p><em>Opening the resource group</em></p>
</li>
<li>
<p>Wait until "Deploying" changes to "Succeeded," indicating that the Data Lake Store has been created. Deployment typically takes a minute or less. You can click the <strong>Refresh</strong> button at the top of the blade to refresh the deployment status.</p>
<p><a href="Images/successful-deployment-1.png" target="_blank"><img src="Images/successful-deployment-1.png" alt="Deployment succeeded" style="max-width:100%;"></a></p>
<p><em>Deployment succeeded</em></p>
</li>
</ol>
<p>Now that you have created a Data Lake Store, the next step is to create a Data Lake Analytics account so you can run queries against the store.</p>
<p><a name="Exercise2"></a></p>
<h2>Exercise 2: Create an Azure Data Lake Analytics account</h2>
<p>Azure Data Lake formally separates the concepts of storing data and querying data. This allows Azure Data Lake Analytics to operate against a range of possible data sources contained in an Azure Data Lake Store. In this exercise, you will create a Data Lake Analytics account and connect it to the Data Lake Store you created in <a href="#Exercise1">Exercise 1</a>.</p>
<ol>
<li>
<p>In the portal, click <strong>+ New</strong>, followed by <strong>Intelligence + analytics</strong> and <strong>Data Lake Analytics</strong>.</p>
<p><a href="Images/new-data-lake-analytics.png" target="_blank"><img src="Images/new-data-lake-analytics.png" alt="Adding a new Data Lake Analytics account" style="max-width:100%;"></a></p>
<p><em>Adding a new Data Lake Analytics account</em></p>
</li>
<li>
<p>In the "New Data Lake Analytics" blade, enter a name for the new account. Once more, the name must be unique across Azure because it becomes part of a DNS name. Select <strong>Use existing</strong> under <strong>Resource Group</strong> and select the resource group that you created in Exercise 1. Then select the same location you selected for the Data Lake Store in Exercise 1. Finally, click <strong>Data Lake Store</strong> and select the Data Lake Store you created in Exercise 1 to associate the Data Lake Analytics account with your Data Lake Store.</p>
<p>When you're finished, click the <strong>Create</strong> button at the bottom of the "New Data Lake Analytics" blade.</p>
<p><a href="Images/create-data-lake-analytics.png" target="_blank"><img src="Images/create-data-lake-analytics.png" alt="Creating a Data Lake Analytics account" style="max-width:100%;"></a></p>
<p><em>Creating a Data Lake Analytics account</em></p>
</li>
<li>
<p>Return to the resource group that holds the Data Lake Store and the Data Lake Analytics account and wait for "Deploying" to change to "Succeeded." Once more, you can click the <strong>Refresh</strong> button at the top of the blade to refresh the deployment status.</p>
<p><a href="Images/successful-deployment-2.png" target="_blank"><img src="Images/successful-deployment-2.png" alt="Deployment succeeded" style="max-width:100%;"></a></p>
<p><em>Deployment succeeded</em></p>
</li>
</ol>
<p>You now have Azure Data Lake storage and query capability set up in your Azure subscription. The next task is to add some data to query.</p>
<p><a name="Exercise3"></a></p>
<h2>Exercise 3: Import data into Azure Data Lake Store</h2>
<p>This lab's "resources" directory holds two tab-delimited TSV files containing sample data. The data comes from the public domain and consists of questions and answers from the popular site <a href="http://academia.stackexchange.com">http://academia.stackexchange.com</a>. In this exercise, you will import the sample data into your Azure Data Lake Store so you can execute queries against it.</p>
<ol>
<li>
<p>In the portal, open the Azure Data Lake Store that you created in <a href="#Exercise1">Exercise 1</a>. (An easy way to do that is to open the resource group and then click the Data Lake Store resource.) In the blade for the Data Lake Store, click <strong>Data Explorer</strong> near the top.</p>
<p><a href="Images/data-explorer.png" target="_blank"><img src="Images/data-explorer.png" alt="Opening Data Explorer" style="max-width:100%;"></a></p>
<p><em>Opening Data Explorer</em></p>
</li>
<li>
<p>Click <strong>Upload</strong>.</p>
<p><a href="Images/data-explorer-upload.png" target="_blank"><img src="Images/data-explorer-upload.png" alt="Opening the &quot;Upload files&quot; blade" style="max-width:100%;"></a></p>
<p><em>Opening the "Upload files" blade</em></p>
</li>
<li>
<p>In the "Upload files" blade, click the <strong>Open</strong> button and select the files named <strong>comments.tsv</strong> and <strong>posts.tsv</strong> in this lab's "resources" directory. Then click <strong>Add selected files</strong>. The combined file size file is more than 90 MB, so the upload might take a few minutes.</p>
<p><a href="Images/data-explorer-upload-tsv.png" target="_blank"><img src="Images/data-explorer-upload-tsv.png" alt="Uploading TSV files" style="max-width:100%;"></a></p>
<p><em>Uploading TSV files</em></p>
</li>
<li>
<p>Once the uploads have finished, close the "Upload files" blade and return to the blade for your Data Lake Store. Confirm that both of the data files you uploaded appear there. Then click <strong>posts.tsv</strong> to view the contents of that file.</p>
<p><a href="Images/data-explorer-uploads-complete.png" target="_blank"><img src="Images/data-explorer-uploads-complete.png" alt="Opening posts.tsv" style="max-width:100%;"></a></p>
<p><em>Opening posts.tsv</em></p>
</li>
<li>
<p>The file preview only shows a portion of the data file. Take a moment to examine the data and familiarize yourself with its content and structure.</p>
<p><a href="Images/file-preview.png" target="_blank"><img src="Images/file-preview.png" alt="Previewing posts.tsv" style="max-width:100%;"></a></p>
<p><em>Previewing  posts.tsv</em></p>
</li>
</ol>
<p>The next step is to query the data to extract the information you want from it. For that, Azure Data Lake Analytics provides U-SQL.</p>
<p><a name="Exercise4"></a></p>
<h2>Exercise 4: Query a TSV file with U-SQL</h2>
<p><a href="http://usql.io/">U-SQL</a> is a language created by Microsoft that combines traditional SQL Data Definition Language (DDL) and Data Manipulation Language (DML) constructs with expressions, functions, and operators based on the popular C# programming language. It marries the benefits of SQL with the power of expressive code. And it is supported natively in Azure Data Lake Analytics. In this exercise, you will use U-SQL to query the data you imported in <a href="#Exercise3">Exercise 3</a>.</p>
<ol>
<li>
<p>In the portal, open the Azure Data Lake Analytics account that you created in <a href="#Exercise2">Exercise 2</a>. In the ensuing blade, click <strong>+ New Job</strong> to create a new U-SQL job.</p>
<p><a href="Images/new-analytics-job.png" target="_blank"><img src="Images/new-analytics-job.png" alt="Creating a new U-SQL job" style="max-width:100%;"></a></p>
<p><em>Creating a new U-SQL job</em></p>
</li>
<li>
<p>In the "New U-SQL Job" blade, paste the following query into the empty query field:</p>
<div class="highlight highlight-source-sql"><pre><span class="pl-k">//</span> here we define the schema for the imported <span class="pl-c1">posts</span>.<span class="pl-c1">tsv</span> file
@posts <span class="pl-k">=</span>
    EXTRACT id                  <span class="pl-k">int</span>,
            [type]              string,
            acceptedanswerid    <span class="pl-k">int</span>?,
            parentquestionid    <span class="pl-k">int</span>?,
            creationdate        string,
            score               <span class="pl-k">int</span>,
            views               <span class="pl-k">int</span>,
            ownerid             <span class="pl-k">int</span>,
            title               string,
            body                string,
            tags                string,
            answers             <span class="pl-k">int</span>,
            comments            <span class="pl-k">int</span>
    <span class="pl-k">FROM</span> <span class="pl-s"><span class="pl-pds">"</span>posts.tsv<span class="pl-pds">"</span></span>
    USING <span class="pl-c1">Extractors</span>.<span class="pl-c1">Tsv</span>();

<span class="pl-k">//</span> here we transform the imported data using various aggregate functions
@results <span class="pl-k">=</span>
    <span class="pl-k">SELECT</span>
        ownerid <span class="pl-k">AS</span> userid,
        <span class="pl-c1">SUM</span>(score) <span class="pl-k">AS</span> totalscore,
        <span class="pl-c1">COUNT</span>(<span class="pl-k">*</span>) <span class="pl-k">AS</span> totalposts
    <span class="pl-k">FROM</span> @posts
<span class="pl-k">GROUP BY</span> ownerid;

<span class="pl-k">//</span> finally we output the transformed data for further analysis <span class="pl-k">or</span> visualization
OUTPUT @results
    TO <span class="pl-s"><span class="pl-pds">"</span>totalscores.csv<span class="pl-pds">"</span></span>
    <span class="pl-k">ORDER BY</span> totalscore <span class="pl-k">DESC</span>
    USING <span class="pl-c1">Outputters</span>.<span class="pl-c1">Csv</span>();</pre></div>
<p>The query contains three main parts. The <strong>EXTRACT</strong> statement extracts data from an existing data source, in this case the <strong>posts.tsv</strong> file you uploaded to the Data Lake Store. The <strong>SELECT</strong> statement transforms the input data into a shape suitable to the task at hand. Finally, the <strong>OUTPUT</strong> statement outputs the result as a named rowset, which can be used for further analysis or visualization.</p>
</li>
<li>
<p>Click <strong>Submit Job</strong> to execute the query.</p>
<p><a href="Images/simple-query.png" target="_blank"><img src="Images/simple-query.png" alt="Executing the query" style="max-width:100%;"></a></p>
<p><em>Executing the query</em></p>
</li>
<li>
<p>A new blade will open to show what is happening as the Data Lake Analytics engine prepares, queues, and executes your query. The job is complete when the "Finalizing" step turns green.</p>
<p><a href="Images/finished-job.png" target="_blank"><img src="Images/finished-job.png" alt="The completed job" style="max-width:100%;"></a></p>
<p><em>The completed job</em></p>
</li>
<li>
<p>Return to the blade for your Data Lake Store and click <strong>Data Explorer</strong>. Then click <strong>totalscores.csv</strong> to view the query results and verify that it contains three columns of data.</p>
<p><a href="Images/total-scores-csv.png" target="_blank"><img src="Images/total-scores-csv.png" alt="Viewing the query results" style="max-width:100%;"></a></p>
<p><em>Viewing the query results</em></p>
</li>
</ol>
<p>In the next two exercises, you will build on what you learned here by joining multiple data sources and performing more complex queries against the aggregated data. Now that you know how to set up a Data Lake Store, import data, connect it to in Data Lake Analytics, and execute U-SQL queries, the fundamentals are in place.</p>
<p><a name="Exercise5"></a></p>
<h2>Exercise 5: Create an Azure SQL Database as a federated data source</h2>
<p>In the previous exercise, you issued a simple query against a single file in an Azure Data Lake Store. To make things more interesting, you are now going to create a SQL database in your Azure subscription and configure it to serve as a federated data source in Data Lake Analytics. This will allow you to not only query the database with U-SQL, but also join data from the database to data already residing in your Data Lake Store. This demonstrates the power of Azure Data Lake as a distributed storage and analytics engine.</p>
<p>Enabling federated queries will require a series of steps:</p>
<ul>
<li>Create an Azure storage account in your Azure subscription</li>
<li>Upload a SQL database backup file (a .bacpac file) to the new storage account</li>
<li>Create a new SQL database in your Azure subscription and restore it from the .bacpac file</li>
<li>Configure your Data Lake Analytics account to query against the database</li>
</ul>
<p>Let's get started!</p>
<ol>
<li>
<p>In the Azure Portal, click <strong>+ New</strong>, followed by <strong>Storage</strong> and <strong>Storage account</strong>.</p>
<p><a href="Images/new-storage-account.png" target="_blank"><img src="Images/new-storage-account.png" alt="Adding a storage account" style="max-width:100%;"></a></p>
<p><em>Adding a storage account</em></p>
</li>
<li>
<p>In the ensuing "Create storage account" blade, enter a name for the new storage account in <strong>Name</strong> field. Storage account names must be 3 to 24 characters in length and can only contain numbers and lowercase letters. In addition, the name you enter must be unique within Azure. If someone else has chosen the same name, you'll be notified that the name isn't available with a red exclamation mark in the <strong>Name</strong> field.</p>
<p>Once you have a name that Azure will accept, select <strong>Use existing</strong> under <strong>Resource group</strong> and select the resource group that you created in Exercise 1. Then select the location that you selected for the Data Lake Store. Finish up by clicking the <strong>Create</strong> button at the bottom of the blade to create the new storage account.</p>
<p><a href="Images/create-storage-account.png" target="_blank"><img src="Images/create-storage-account.png" alt="Creating a new storage account" style="max-width:100%;"></a></p>
<p><em>Creating a new storage account</em></p>
</li>
<li>
<p>Click <strong>Resource groups</strong> in the ribbon on the left side of the portal, and then click the resource group that holds the storage account. Now click the storage account to open a blade for that account.</p>
<p><a href="Images/open-storage-account.png" target="_blank"><img src="Images/open-storage-account.png" alt="Opening the storage account" style="max-width:100%;"></a></p>
<p><em>Opening the storage account</em></p>
</li>
<li>
<p>You need to create a container in the storage account to hold the database backup. To begin, click <strong>Blobs</strong> in the storage account's blade.</p>
<p><a href="Images/open-blob-storage.png" target="_blank"><img src="Images/open-blob-storage.png" alt="Opening blob storage" style="max-width:100%;"></a></p>
<p><em>Opening blob storage</em></p>
</li>
<li>
<p>Click <strong>+ Container</strong> to create a new container.</p>
<p><a href="Images/add-container.png" target="_blank"><img src="Images/add-container.png" alt="Adding a container" style="max-width:100%;"></a></p>
<p><em>Adding a container</em></p>
</li>
<li>
<p>Enter the name "bacpacs" (without quotation marks) for the new blob container, and then click <strong>Create</strong>:</p>
<p><a href="Images/new-container.png" target="_blank"><img src="Images/new-container.png" alt="Creating a new container" style="max-width:100%;"></a></p>
<p><em>Creating a new container</em></p>
</li>
<li>
<p>Click the "bacpacs" container to open it.</p>
<p><a href="Images/open-container.png" target="_blank"><img src="Images/open-container.png" alt="Opening the &quot;bacpacs&quot; container" style="max-width:100%;"></a></p>
<p><em>Opening the "bacpacs" container</em></p>
</li>
<li>
<p>Click <strong>Upload</strong>.</p>
<p><a href="Images/upload-blob-1.png" target="_blank"><img src="Images/upload-blob-1.png" alt="Uploading to the &quot;bacpacs&quot; container" style="max-width:100%;"></a></p>
<p><em>Uploading to the "bacpacs" container</em></p>
</li>
<li>
<p>Click the <strong>Open</strong> button to the right of the <strong>Files</strong> box. Navigate to this lab's "resources" folder and select the file named <strong>academics-stackexchange-users.bacpac</strong>. Then click the <strong>Upload</strong> button to upload the file to the container.</p>
<p><a href="Images/upload-blob-2.png" target="_blank"><img src="Images/upload-blob-2.png" alt="Uploading a file" style="max-width:100%;"></a></p>
<p><em>Uploading a file</em></p>
</li>
<li>
<p>Wait until the upload has completed. Then close the "Upload blob" blade and return to the blade for the "bacpacs" container. Confirm that the container now contains a blob named <strong>academics-stackexchange-users.bacpac</strong>.</p>
<p><a href="Images/new-bacpac-blob.png" target="_blank"><img src="Images/new-bacpac-blob.png" alt="The uploaded blob" style="max-width:100%;"></a></p>
<p><em>The uploaded blob</em></p>
</li>
<li>
<p>The next step is to create a new SQL database server. In the Azure portal, click <strong>More services</strong> in the ribbon on the left and type "sql" (without quotation marks) in the search box. Then click <strong>SQL servers</strong>:</p>
<p><a href="Images/sql-servers.png" target="_blank"><img src="Images/sql-servers.png" alt="Searching for SQL servers" style="max-width:100%;"></a></p>
<p><em>Searching for SQL servers</em></p>
</li>
<li>
<p>Click <strong>+ Add</strong> in the "SQL servers" blade:</p>
<p><a href="Images/add-sql-server.png" target="_blank"><img src="Images/add-sql-server.png" alt="Adding a SQL server" style="max-width:100%;"></a></p>
<p><em>Adding a SQL server</em></p>
</li>
<li>
<p>Enter a unique name for your SQL server. (It must be unique across all of Azure; be sure a green check mark appears in the box.) Enter "azureuser" (without quotation marks) as the user name, and "Azure4Research!" (again without quotation marks) as the password. Under <strong>Resource group</strong>, select <strong>Use existing</strong> and select the same resource group you have used throughout this lab. For <strong>Location</strong>, select the same location you selected in previous exercises. When you're finished, click the <strong>Create</strong> button at the bottom of the blade.</p>
<p><a href="Images/create-sql-server.png" target="_blank"><img src="Images/create-sql-server.png" alt="Creating a new SQL server" style="max-width:100%;"></a></p>
<p><em>Creating a new SQL server</em></p>
<p>After a few moments, the SQL server will be created. Click the <strong>Refresh</strong> button at the top of the "SQL servers" blade and make sure the new SQL server appears in the list of SQL servers associated with your subscription.</p>
</li>
<li>
<p>Next, you need to create a new database instance using the blob you uploaded a few moments ago. In the "SQL servers" blade, click the SQL server you just created. Then click <strong>Import database</strong>.</p>
<p><a href="Images/import-database.png" target="_blank"><img src="Images/import-database.png" alt="Importing a database" style="max-width:100%;"></a></p>
<p><em>Importing a database</em></p>
</li>
<li>
<p>In the "Import database" blade, click <strong>Storage</strong> and select the storage account that you uploaded the .bacpac file to, followed by the "bacpacs" container and, after that, the blob you uploaded to that container. Then click the <strong>Select</strong> button at the bottom of that blade. Now return to the "Import database" blade and enter "azureuser" as the user name and "Azure4Research!" as the password, both without quotation marks. Finish up by clicking <strong>OK</strong> at the bottom of the blade.</p>
<p><a href="Images/import-database-instance.png" target="_blank"><img src="Images/import-database-instance.png" alt="Specifying database import options" style="max-width:100%;"></a></p>
<p><em>Specifying database import options</em></p>
</li>
<li>
<p>While you wait for the database instance to be created, click <strong>Show firewall settings</strong> in the database-server blade.</p>
<p><a href="Images/show-firewall-settings.png" target="_blank"><img src="Images/show-firewall-settings.png" alt="Viewing firewall settings" style="max-width:100%;"></a></p>
<p><em>Viewing firewall settings</em></p>
</li>
<li>
<p>Add an IP range to allow Data Lake Analytics to communicate with your server during federated query execution. Type the following values into the three text boxes and then click <strong>Save</strong> at the top of the blade:</p>
<ul>
<li><strong>Rule Name</strong>: Allow Data Lake</li>
<li><strong>Start IP</strong>: 25.66.0.0</li>
<li><strong>End IP</strong>: 25.66.255.255</li>
</ul>
<p><a href="Images/allow-port-range.png" target="_blank"><img src="Images/allow-port-range.png" alt="Configuring the firewall" style="max-width:100%;"></a></p>
<p><em>Configuring the firewall</em></p>
</li>
<li>
<p>Now that you have a SQL database instance up and running, the final step is to register it with Data Lake Analytics for federation. Navigate back to your Data Lake Analytics account in the portal and click <strong>+ New Job</strong> at the top of the blade. In the "New U-SQL Job" blade, enter the following statement and then click <strong>Submit Job</strong> to run the job:</p>
<div class="highlight highlight-source-sql"><pre><span class="pl-k">CREATE</span> <span class="pl-k">DATABASE</span> <span class="pl-en">UserIntegration</span>;</pre></div>
</li>
<li>
<p>If you haven't installed the <a href="https://azure.microsoft.com/en-us/documentation/articles/xplat-cli-install/">Azure CLI</a>, take a moment to install it now. Then open a command shell (Bash, Terminal, Command Prompt, etc.) and execute the following command:</p>
<pre><code>azure login
</code></pre>
</li>
<li>
<p>Copy the access code presented to you in the command shell. Then open a browser window and navigate to <a href="https://aka.ms/devicelogin">https://aka.ms/devicelogin</a> and enter the code. If prompted to log in, do so using your Microsoft account. Upon successful authentication, your command-line session will be connected to your Azure subscription.</p>
</li>
<li>
<p>Assuming you are using the Azure Pass subscription provided to you for these labs, execute the following command to ensure that Azure Pass is the active subscription (the subscription that will be charged against) for operations performed with the CLI:</p>
<pre><code>azure account set "Azure Pass"
</code></pre>
</li>
<li>
<p>Now execute the following commands to create a Data Lake catalog credential used to authenticate when executing federated queries. Substitute your Data Lake Analytics account name for <em>analytics_account_name</em> and your database server name (the one specified in Step 13 of this exercise) for <em>database_server_name</em>:</p>
 <pre> azure config mode arm
 azure datalake analytics catalog credential create <i>analytics_account_name</i> UserIntegration tcp://<i>database_server_name</i>.database.windows.net FederatedCredential azureuser</pre>
<p>When prompted for a password, enter the SQL server password ("Azure4Research!") you specified in Step 13.</p>
</li>
<li>
<p>Return to your Data Lake Analytics account in the Azure Portal. Then click <strong>+ New Job</strong> and execute the following query:</p>
<div class="highlight highlight-source-sql"><pre>USE DATABASE UserIntegration;

CREATE DATA SOURCE IF NOT EXISTS AcademicSEDb <span class="pl-k">FROM</span> AZURESQLDB WITH
   ( PROVIDER_STRING <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Database=academics-stackexchange-users;Trusted_Connection=False;Encrypt=True<span class="pl-pds">"</span></span>,
     CREDENTIAL <span class="pl-k">=</span> FederatedCredential,
     REMOTABLE_TYPES <span class="pl-k">=</span> (bool, byte, sbyte, short, ushort, <span class="pl-k">int</span>, uint, long, ulong, <span class="pl-k">decimal</span>, float, double, string, DateTime) );

CREATE EXTERNAL TABLE User (
                        [id] <span class="pl-k">int</span>,
                        [reputation] <span class="pl-k">int</span>,
                        [created] DateTime,
                        [displayname] string,
                        [lastaccess] DateTime,
                        [location] string
                    ) <span class="pl-k">FROM</span> AcademicSEDb LOCATION <span class="pl-s"><span class="pl-pds">"</span>dbo.User<span class="pl-pds">"</span></span>;</pre></div>
<p>This query configures your SQL database as a data source authenticated with the credential you created in Step 22, and then creates a named table in your local Data Lake Analytics database which is backed by the SQL data source.</p>
</li>
</ol>
<p>That was a lot of work, but you are now ready to issue federated queries. Let's try it out!</p>
<p><a name="Exercise6"></a></p>
<h2>Exercise 6: Perform a federated query with U-SQL</h2>
<p>Two of the most compelling features of Data Lake Analytics are its ability to federate external data sources (meaning, query them in their native storage, with copying) and its ability to address multiple disparate data sources in a single query. In this exercise, you'll use both to join data from the SQL database you created in <a href="#Exercise5">Exercise 5</a> with data in one of the tab-delimited files you imported in <a href="#Exercise3">Exercise 3</a>.</p>
<ol>
<li>
<p>In the Azure Portal, navigate to your Data Lake Analytics account and click <strong>+ New Job</strong>. Paste the following query into the query-text field and click <strong>Submit Job</strong> to run the job.</p>
<div class="highlight highlight-source-sql"><pre>USE DATABASE UserIntegration;

<span class="pl-k">//</span> here we define the schema for the imported <span class="pl-c1">posts</span>.<span class="pl-c1">tsv</span> file
@posts <span class="pl-k">=</span>
    EXTRACT id                  <span class="pl-k">int</span>,
            [type]              string,
            acceptedanswerid    <span class="pl-k">int</span>?,
            parentquestionid    <span class="pl-k">int</span>?,
            creationdate        string,
            score               <span class="pl-k">int</span>,
            views               <span class="pl-k">int</span>,
            ownerid             <span class="pl-k">int</span>,
            title               string,
            body                string,
            tags                string,
            answers             <span class="pl-k">int</span>,
            comments            <span class="pl-k">int</span>
    <span class="pl-k">FROM</span> <span class="pl-s"><span class="pl-pds">"</span>posts.tsv<span class="pl-pds">"</span></span>
    USING <span class="pl-c1">Extractors</span>.<span class="pl-c1">Tsv</span>();

<span class="pl-k">//</span> here we find the earliest post <span class="pl-k">date</span> per user... note the C<span class="pl-c"><span class="pl-c">#</span> date conversion</span>
@earliest_posts <span class="pl-k">=</span>
    <span class="pl-k">SELECT</span>
        ownerid,
        <span class="pl-c1">MIN</span>(<span class="pl-c1">DateTime</span>.<span class="pl-c1">Parse</span>(creationdate)) <span class="pl-k">AS</span> created
    <span class="pl-k">FROM</span> @posts
<span class="pl-k">GROUP BY</span> ownerid;

<span class="pl-k">//</span> now we <span class="pl-k">join</span> to the external SQL Database table to add user names to the output
@results <span class="pl-k">=</span>
    <span class="pl-k">SELECT</span>
        u.[displayname] <span class="pl-k">AS</span> [name],
        ep.[created] <span class="pl-k">AS</span> [first_post_date]
    <span class="pl-k">FROM</span>
        User <span class="pl-k">AS</span> u
            <span class="pl-k">INNER JOIN</span> @earliest_posts <span class="pl-k">AS</span> ep <span class="pl-k">ON</span> ep.[ownerid] <span class="pl-k">==</span> u.[id];

<span class="pl-k">//</span> finally we output the transformed data for further analysis <span class="pl-k">or</span> visualization
OUTPUT @results
    TO <span class="pl-s"><span class="pl-pds">"</span>firstposts.csv<span class="pl-pds">"</span></span>
    USING <span class="pl-c1">Outputters</span>.<span class="pl-c1">Csv</span>();</pre></div>
<p>This query first applies structure to the data in <strong>posts.tsv</strong> and then queries that file for the earliest post by each user. Then it joins the query results to the external database table created in the previous exercise and performs another query to associate a user name with each post. Finally, it writes the output of this query to <strong>firstposts.csv</strong>. Note the call to <strong>DateTime.Parse</strong> embedded in the query. This is an example of how C# expressions can be included in U-SQL to richen the queries you perform.</p>
</li>
<li>
<p>Once the job has run successfully, open the blade for your Data Lake Store and click <strong>Data Explorer</strong> near the top. Confirm that the Data Lake Store contains a file named <strong>firstposts.csv</strong>. Then click the file.</p>
<p><a href="Images/first-posts-csv.png" target="_blank"><img src="Images/first-posts-csv.png" alt="Viewing the query results" style="max-width:100%;"></a></p>
<p><em>Viewing the query results</em></p>
</li>
<li>
<p>Confirm that the file contains two columns of data: one containing the name of each user who posted in the discussion forum, and another containing the time and date of each user's first post.</p>
<p><a href="Images/query-results.png" target="_blank"><img src="Images/query-results.png" alt="The query results" style="max-width:100%;"></a></p>
<p><em>The query results</em></p>
</li>
</ol>
<p>You just demonstrated that U-SQL can be used to query multiple data sources of different types. You also saw one example of how C# expressions can be used to richen queries in U-SQL.</p>
<h2>Summary</h2>
<p>Azure Data Lake provides a hyperscale, enterprise-wide repository in which different types of data can be collected without regard to size, structure, or velocity. Once aggregated in a Data Lake Store, data can be analyzed with Azure Data Lake Analytics, or processed with popular open-source tools such as Apache Hadoop and Apache Spark hosted in <a href="https://azure.microsoft.com/en-us/services/hdinsight/">Azure HDInsight</a>. In this lab, you learned how to import various types of data into a Data Lake Store and use Azure Data Lake Analytics to query the combined data with U-SQL.</p>
<p>Azure Data Lake does not itself provide tools for visualizing query results, but other components of Azure and the Azure ecosystem do. For example, <a href="https://powerbi.microsoft.com/en-us/">Microsoft Power BI</a> can be used to visualize query results and can even be connected directly to a Data Lake Store. For more information about combining Azure Data Lake with Power BI and a tutorial to help guide the way, see <a href="https://azure.microsoft.com/en-us/documentation/articles/data-lake-store-power-bi/">Analyze data in Data Lake Store by using Power BI</a>.</p>
<hr>
<p>Copyright 2017 Microsoft Corporation. All rights reserved. Except where otherwise noted, these materials are licensed under the terms of the MIT License. You may use them according to the license as is most appropriate for your project. The terms of this license can be found at <a href="https://opensource.org/licenses/MIT">https://opensource.org/licenses/MIT</a>.</p>
</body>
</html>
<!-- This document was created with MarkdownPad, the Markdown editor for Windows (http://markdownpad.com) -->
