{"nbformat_minor": 0, "cells": [{"source": "# Exercise 5: Machine Learning with Spark\n\nThis Notebook will introduce you to the Machine Learning tools available in Spark and will show how you can use them to train and evaluate a model based on the data that you are examining.", "cell_type": "markdown", "metadata": {}}, {"source": "#### Step 1 - Accessing the data\n\nFor this lab, you will once again be working with the Food Inspection data collected by the City of Chicago.  Thus the steps that you will follow to access the data are effectively the same as they were in the previous exercise.\n\nRun the code in the following cells.  This code will prepare the `inspectionDataframe` with the Food Inspection data that you will be learning during this Machine Learning exercise.\n\n*(Note - this very first cell run might take a little extra time, as environment resources are being provisioned.)*", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Obtain a reference to the text file\nfilename = 'wasbs://sparklab@a4rsparkresources.blob.core.windows.net/FoodInspectionsNoHeader.csv'\ninspectionsFile = sc.textFile(filename)\n\n# Process each line of the CSV file by mapping them through a parse function.\ndef csvParse(s):\n    import csv\n    from StringIO import StringIO\n    sio = StringIO(s)\n    value = csv.reader(sio).next()\n    sio.close()\n    return value\ninspectionDataset = inspectionsFile.map(csvParse)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.sql.types import *\n\n# Configure the schema that describes the data being imported \nschema=StructType([\n        StructField('Inspection ID', StringType(), False),\n        StructField('DBA Name', StringType(), False),\n        StructField('AKA Name', StringType(), True),\n        StructField('License#', StringType(), False),\n        StructField('FacilityType', StringType(), True),\n        StructField('Risk', StringType(), True),\n        StructField('Address', StringType(), True),\n        StructField('City', StringType(), True),\n        StructField('State', StringType(), True),\n        StructField('Zip', StringType(), True),\n        StructField('InspectionDate', StringType(), False),\n        StructField('InspectionType', StringType(), False),\n        StructField('Results', StringType(), False),\n        StructField('Latitude', StringType(), True),\n        StructField('Longitude', StringType(), True)])\n\n# Create a dataframe by bringing the data and the schema together\ninspectionDataframe = sqlContext.createDataFrame(inspectionDataset, schema)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### Step 2 - Prepare the data\n\nIn general, the first step in a Machine Learning exercise (once the data has been acquired) is to pre-process the data in order to align the data content with the machine learning tools being used.  This sometimes includes replacing or removing extraneous or outlying data, identifying label and feature values, and other preparatory steps.\n\nThe following several cells perform the data preparation tasks necessary for this lab.  These steps include:\n- Reducing the inspection results to a pass/fail/other category, then dropping rows from the \"other\" category in order to get to a set of binary pass/fail records.\n- Mapping text-based feature values to numerical values required by the Spark ML Machine Learning algorithms.\n- Grouping the features to be used by Spark ML to develop a model.\n\nRun the code in the following three cells to perform these data preparation tasks.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.ml import Pipeline\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import UserDefinedFunction\n\n# Function to map Pass/Fail to 1.0 or 0.0 (and -1 for other values)\ndef labelForResults(s):\n    if s == 'Fail':\n        return 0.0\n    elif s == 'Pass w/ Conditions' or s == 'Pass':\n        return 1.0\n    else:\n        return -1.0\nlabel = UserDefinedFunction(labelForResults, DoubleType())\n\n# Create a new dataframe that uses the computed \"label\" instead of the \"results\", and excludes \"other values\"\nlabeledData = inspectionDataframe.select(\n    label(inspectionDataframe['results']).alias('label'), \n    inspectionDataframe['FacilityType'], \n    inspectionDataframe['InspectionType'], \n    inspectionDataframe['Zip']).where('label >= 0')\n\nprint('After filtering, there are ' + str(labeledData.count()) + ' records.')", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Use indexers to convert from string values to a numeric index value\nfrom pyspark.ml.feature import StringIndexer\n\nfacilityIndexer = StringIndexer(inputCol=\"FacilityType\", outputCol=\"FacilityTypeIndex\")\ninspectionIndexer = StringIndexer(inputCol=\"InspectionType\", outputCol=\"InspectionTypeIndex\")\nzipIndexer = StringIndexer(inputCol=\"Zip\", outputCol=\"ZipIndex\")\n\n# Run the indexers to create a new dataframe\npipeline = Pipeline(stages=[facilityIndexer, inspectionIndexer, zipIndexer])\nindexedData = pipeline.fit(labeledData).transform(labeledData)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Convert from several discrete feature columns to a single vector feature column\nfrom pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(inputCols=[\"FacilityTypeIndex\", \"InspectionTypeIndex\", \"ZipIndex\"], outputCol=\"features\")\npreparedData = assembler.transform(indexedData)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### Step 3 - Train and Score the Model\n\nOnce the data has been prepared, you can then build, train, and score your model.  To do this, you generally need to identify two data sets.  The first data set will be used for training the model.  The second data set is usuallyy independent of the first one sand is used to score the model - by applying the Machine Learning model to each of the values in the scoring data set and seeing how effective it is at predicting the outcome.\n\nRun the code in the following three cells.  The first cell splits the data that you have prepared 80%/20% into two other collections, one for training and one for scoring.  The second cell creates an instance of the *Naive Bayes*  classifer Machine Learning algorithm and trains it with your training data.  Finally, the third cell applies your trained model to the scoring data and prints out the relative success percentage.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Split the sample data into 80% training set, 20% scoring/evaluation set\nsplits = preparedData.randomSplit([0.8, 0.2], 24)\ntrainingData = splits[0]\nscoringData = splits[1]\nprint(\"Training: \" + str(trainingData.count()) + ' records.')\nprint(\"Scoring: \" + str(scoringData.count()) + ' records.')", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Train the ML model\nfrom pyspark.ml.classification import NaiveBayes\n\nnb = NaiveBayes()\nnbModel = nb.fit(trainingData)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Score/evaluate the model\npredictions = nbModel.transform(scoringData)\npredictions.registerTempTable('predictions')\n\n# Display the success rate\nnumSuccesses = predictions.where(\"\"\"(prediction = 0 AND label = 0) OR \n                                      (prediction = 1 AND label = 1)\"\"\").count()\nnumInspections = predictions.count()\n\nprint \"There were\", numInspections, \"inspections and there were\", numSuccesses, \"successful predictions\"\nprint \"This is a\", str((float(numSuccesses) / float(numInspections)) * 100) + \"%\", \"success rate\"", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### Step 4 - Graph the model's accuracy\nThe accuracy percentage text displayed at the conclusion of the previous step is interesting, but it might be more useful to actually visualize the success of the algorithm.\n\nRun the code in the following five cells. The first four use the `%%sql` Magic Instruction to calculate the number of true and false positive and negative results that resulted from scoring your Machine Learning algorithm.  The code in the final cell will then plot these values in a pie chart so you can visualize the strengths and weaknesses of your trained model.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%sql -q -o true_positive\nSELECT count(*) AS cnt FROM Predictions WHERE prediction = 1 AND label = 1", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "%%sql -q -o false_positive\nSELECT count(*) AS cnt FROM Predictions WHERE prediction = 1 AND label = 0", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "%%sql -q -o true_negative\nSELECT count(*) AS cnt FROM Predictions WHERE prediction = 0 AND label = 0", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "%%sql -q -o false_negative\nSELECT count(*) AS cnt FROM Predictions WHERE prediction = 0 AND label = 1", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nlabels = ['True positive', 'False positive', 'True negative', 'False negative']\nsizes = [true_positive['cnt'], false_positive['cnt'], false_negative['cnt'], true_negative['cnt']]\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.axis('equal')", "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "At this point, you could work on interpreting the chart above and use the insight it provides to refine your data collection, preparation, and training work until you have a Machine Learning model whose accuracy you are satisfied with.", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"name": "python"}}}}